{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prospective-america",
   "metadata": {},
   "source": [
    "Dado que el entrenamiento de redes neuronales es una tarea  muy costosa, **se recomienda ejecutar el notebooks en [Google Colab](https://colab.research.google.com)**, por supuesto también se puede ejecutar en local.\n",
    "\n",
    "Al entrar en [Google Colab](https://colab.research.google.com) bastará con hacer click en `upload` y subir este notebook. No olvide luego descargarlo en `File->Download .ipynb`\n",
    "\n",
    "**El examen deberá ser entregado con las celdas ejecutadas, si alguna celda no está ejecutadas no se contará.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-stewart",
   "metadata": {},
   "source": [
    "El examen se divide en preguntas de código y preguntas teóricas, con la puntuación que se indica a continuación. La puntuación máxima será 10.\n",
    "\n",
    "- [Actividad 1: Redes Densas](#actividad_1): 10 pts\n",
    "    - Correcta normalización: máximo de 0.5 pts\n",
    "    - [Cuestión 1](#1.1): 1.5 pts\n",
    "    - [Cuestión 2](#1.2): 1.5 pts\n",
    "    - [Cuestión 3](#1.3): 1.5 pts\n",
    "    - [Cuestión 4](#1.4): 1 pts\n",
    "    - [Cuestión 5](#1.5): 1 pts\n",
    "    - [Cuestión 6](#1.6): 1 pts\n",
    "    - [Cuestión 7](#1.7): 1 pts\n",
    "    - [Cuestión 8](#1.8): 1 pts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "prompt-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-correction",
   "metadata": {},
   "source": [
    "<a name='actividad_1'></a>\n",
    "# Actividad 1: Redes Densas\n",
    "\n",
    "Para esta actividad vamos a utilizar el [wine quality dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality). Con el que trataremos de predecir la calidad del vino.\n",
    "\n",
    "La calidad del vino puede tomar valores decimales (por ejemplo 7.25), independientemente de que en el dataset de entrenamiento sean números enteros. Por lo tanto, el problema es una `regresión`.\n",
    "\n",
    "**Puntuación**: \n",
    "\n",
    "Normalizar las features correctamente (x_train, x_test): 0.5 pts \n",
    "\n",
    "- Correcta normalización: máximo de 0.5 pts\n",
    "- [Cuestión 1](#1.1): 1 pt\n",
    "- [Cuestión 2](#1.2): 1 pt\n",
    "- [Cuestión 3](#1.3): 0.5 pts\n",
    "- [Cuestión 4](#1.4): 0.5 pts\n",
    "- [Cuestión 5](#1.5): 0.5 pts\n",
    "- [Cuestión 6](#1.6): 0.5 pts\n",
    "- [Cuestión 7](#1.7): 0.5 pts\n",
    "- [Cuestión 8](#1.8): 0.5 pts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "presidential-milan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar los datos con pandas\n",
    "df_red = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv',\n",
    "    sep=';'\n",
    ")\n",
    "df_white = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv',\n",
    "    sep=';'\n",
    ")\n",
    "df = pd.concat([df_red, df_white])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7c2f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', \n",
    "    'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol'\n",
    "]\n",
    "\n",
    "\n",
    "# separar features y target\n",
    "y = df.pop('quality').values\n",
    "X = df.copy().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f4adb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train, y_train shapes: (4872, 11) (4872,)\n",
      "x_test, y_test shapes: (1625, 11) (1625,)\n",
      "Some qualities:  [6 7 8 5 6]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "print('x_train, y_train shapes:', x_train.shape, y_train.shape)\n",
    "print('x_test, y_test shapes:', x_test.shape, y_test.shape)\n",
    "print('Some qualities: ', y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "painted-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Si quiere, puede normalizar las features\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-planner",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "## Cuestión 1: Cree un modelo secuencial que contenga 4 capas ocultas(hidden layers), con más de 60 neuronas  por capa, sin regularización y obtenga los resultados.\n",
    "\n",
    "Puntuación: \n",
    "- Obtener el modelo correcto: 0.8 pts\n",
    "- Compilar el modelo: 0.1pts\n",
    "- Acertar con la función de pérdida: 0.1 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "working-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "# Código aquí\n",
    " \n",
    "model = Sequential([\n",
    "    Dense(61, activation='relu', input_shape=(11,)),  # Asumiendo que hay 11 características de entrada\n",
    "    Dense(61, activation='relu'),\n",
    "    Dense(61, activation='relu'),\n",
    "    Dense(61, activation='relu'),\n",
    "    Dense(1, activation='linear')  # Asumiendo que es una regresión\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mobile-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del modelo\n",
    "# Código aquí\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')  # Usar mean_squared_error para problemas de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "rotary-credits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "122/122 [==============================] - 2s 5ms/step - loss: 5.5254 - val_loss: 1.7176\n",
      "Epoch 2/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 1.1340 - val_loss: 1.0309\n",
      "Epoch 3/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.7476 - val_loss: 0.7319\n",
      "Epoch 4/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5906 - val_loss: 0.6271\n",
      "Epoch 5/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.5258 - val_loss: 0.6044\n",
      "Epoch 6/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.5113 - val_loss: 0.5530\n",
      "Epoch 7/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.4755 - val_loss: 0.5641\n",
      "Epoch 8/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.4669 - val_loss: 0.5148\n",
      "Epoch 9/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.4513 - val_loss: 0.5276\n",
      "Epoch 10/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.4582 - val_loss: 0.5534\n",
      "Epoch 11/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.4407 - val_loss: 0.5626\n",
      "Epoch 12/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.4524 - val_loss: 0.4885\n",
      "Epoch 13/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.4386 - val_loss: 0.5027\n",
      "Epoch 14/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.4319 - val_loss: 0.4754\n",
      "Epoch 15/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.4228 - val_loss: 0.5112\n",
      "Epoch 16/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.4209 - val_loss: 0.4763\n",
      "Epoch 17/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.4068 - val_loss: 0.4892\n",
      "Epoch 18/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.4065 - val_loss: 0.4867\n",
      "Epoch 19/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.4041 - val_loss: 0.4786\n",
      "Epoch 20/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.4050 - val_loss: 0.5015\n",
      "Epoch 21/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.4101 - val_loss: 0.4970\n",
      "Epoch 22/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.3883 - val_loss: 0.5288\n",
      "Epoch 23/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.3931 - val_loss: 0.4737\n",
      "Epoch 24/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.3773 - val_loss: 0.5205\n",
      "Epoch 25/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.3848 - val_loss: 0.5100\n",
      "Epoch 26/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.3729 - val_loss: 0.4863\n",
      "Epoch 27/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.3794 - val_loss: 0.4799\n",
      "Epoch 28/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.3649 - val_loss: 0.4910\n",
      "Epoch 29/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.3588 - val_loss: 0.5035\n",
      "Epoch 30/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.3623 - val_loss: 0.4781\n",
      "Epoch 31/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.3468 - val_loss: 0.4758\n",
      "Epoch 32/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.3548 - val_loss: 0.4917\n",
      "Epoch 33/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.3440 - val_loss: 0.4877\n",
      "Epoch 34/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.3338 - val_loss: 0.4832\n",
      "Epoch 35/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.3325 - val_loss: 0.4816\n",
      "Epoch 36/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.3325 - val_loss: 0.5032\n",
      "Epoch 37/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.3431 - val_loss: 0.5115\n",
      "Epoch 38/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.3281 - val_loss: 0.5023\n",
      "Epoch 39/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.3166 - val_loss: 0.5076\n",
      "Epoch 40/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.3157 - val_loss: 0.5224\n",
      "Epoch 41/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.3249 - val_loss: 0.5198\n",
      "Epoch 42/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.3111 - val_loss: 0.5116\n",
      "Epoch 43/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.3003 - val_loss: 0.5254\n",
      "Epoch 44/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.3009 - val_loss: 0.4945\n",
      "Epoch 45/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.2972 - val_loss: 0.5029\n",
      "Epoch 46/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.2857 - val_loss: 0.5470\n",
      "Epoch 47/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.2960 - val_loss: 0.4986\n",
      "Epoch 48/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.2938 - val_loss: 0.5314\n",
      "Epoch 49/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.3002 - val_loss: 0.6433\n",
      "Epoch 50/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.2848 - val_loss: 0.5161\n",
      "Epoch 51/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.2827 - val_loss: 0.5113\n",
      "Epoch 52/200\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.2750 - val_loss: 0.5461\n",
      "Epoch 53/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.2808 - val_loss: 0.5091\n",
      "Epoch 54/200\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.2684 - val_loss: 0.5237\n",
      "Epoch 55/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.2592 - val_loss: 0.5284\n",
      "Epoch 56/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.2744 - val_loss: 0.5189\n",
      "Epoch 57/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.2690 - val_loss: 0.5150\n",
      "Epoch 58/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.2529 - val_loss: 0.5494\n",
      "Epoch 59/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.2565 - val_loss: 0.5417\n",
      "Epoch 60/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.2514 - val_loss: 0.5325\n",
      "Epoch 61/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.2415 - val_loss: 0.5620\n",
      "Epoch 62/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.2391 - val_loss: 0.5655\n",
      "Epoch 63/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.2438 - val_loss: 0.5328\n",
      "Epoch 64/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.2420 - val_loss: 0.5153\n",
      "Epoch 65/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.2297 - val_loss: 0.5192\n",
      "Epoch 66/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.2381 - val_loss: 0.5726\n",
      "Epoch 67/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.2315 - val_loss: 0.5382\n",
      "Epoch 68/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.2340 - val_loss: 0.5449\n",
      "Epoch 69/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.2290 - val_loss: 0.5404\n",
      "Epoch 70/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.2263 - val_loss: 0.5465\n",
      "Epoch 71/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.2082 - val_loss: 0.5410\n",
      "Epoch 72/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.2271 - val_loss: 0.5618\n",
      "Epoch 73/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.2202 - val_loss: 0.5812\n",
      "Epoch 74/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.2134 - val_loss: 0.5487\n",
      "Epoch 75/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.2179 - val_loss: 0.5300\n",
      "Epoch 76/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.2000 - val_loss: 0.5583\n",
      "Epoch 77/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1978 - val_loss: 0.5254\n",
      "Epoch 78/200\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.2016 - val_loss: 0.5601\n",
      "Epoch 79/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.2031 - val_loss: 0.5478\n",
      "Epoch 80/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.1918 - val_loss: 0.5339\n",
      "Epoch 81/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1896 - val_loss: 0.5416\n",
      "Epoch 82/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.2002 - val_loss: 0.5577\n",
      "Epoch 83/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1933 - val_loss: 0.5444\n",
      "Epoch 84/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1798 - val_loss: 0.5579\n",
      "Epoch 85/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1875 - val_loss: 0.5862\n",
      "Epoch 86/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1856 - val_loss: 0.5457\n",
      "Epoch 87/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1848 - val_loss: 0.5558\n",
      "Epoch 88/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1827 - val_loss: 0.5359\n",
      "Epoch 89/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1686 - val_loss: 0.6221\n",
      "Epoch 90/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1909 - val_loss: 0.5634\n",
      "Epoch 91/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1686 - val_loss: 0.5594\n",
      "Epoch 92/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1675 - val_loss: 0.5752\n",
      "Epoch 93/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1680 - val_loss: 0.6027\n",
      "Epoch 94/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1731 - val_loss: 0.5855\n",
      "Epoch 95/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1683 - val_loss: 0.5742\n",
      "Epoch 96/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.1618 - val_loss: 0.5895\n",
      "Epoch 97/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1591 - val_loss: 0.5811\n",
      "Epoch 98/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1643 - val_loss: 0.5658\n",
      "Epoch 99/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1535 - val_loss: 0.5564\n",
      "Epoch 100/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.1538 - val_loss: 0.5455\n",
      "Epoch 101/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1521 - val_loss: 0.5755\n",
      "Epoch 102/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1567 - val_loss: 0.5713\n",
      "Epoch 103/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.1519 - val_loss: 0.5777\n",
      "Epoch 104/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.1499 - val_loss: 0.5644\n",
      "Epoch 105/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.1585 - val_loss: 0.5739\n",
      "Epoch 106/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.1411 - val_loss: 0.5799\n",
      "Epoch 107/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.1409 - val_loss: 0.5919\n",
      "Epoch 108/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.1521 - val_loss: 0.5625\n",
      "Epoch 109/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1393 - val_loss: 0.5942\n",
      "Epoch 110/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1313 - val_loss: 0.5784\n",
      "Epoch 111/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1388 - val_loss: 0.5892\n",
      "Epoch 112/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.1366 - val_loss: 0.5948\n",
      "Epoch 113/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1381 - val_loss: 0.5722\n",
      "Epoch 114/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1301 - val_loss: 0.6318\n",
      "Epoch 115/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1319 - val_loss: 0.5853\n",
      "Epoch 116/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1185 - val_loss: 0.5845\n",
      "Epoch 117/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1202 - val_loss: 0.6167\n",
      "Epoch 118/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1263 - val_loss: 0.5894\n",
      "Epoch 119/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1235 - val_loss: 0.5971\n",
      "Epoch 120/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1308 - val_loss: 0.5781\n",
      "Epoch 121/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1210 - val_loss: 0.5836\n",
      "Epoch 122/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1134 - val_loss: 0.5684\n",
      "Epoch 123/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.1221 - val_loss: 0.5907\n",
      "Epoch 124/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1307 - val_loss: 0.6063\n",
      "Epoch 125/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1338 - val_loss: 0.6007\n",
      "Epoch 126/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1184 - val_loss: 0.5943\n",
      "Epoch 127/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1121 - val_loss: 0.5881\n",
      "Epoch 128/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1164 - val_loss: 0.6078\n",
      "Epoch 129/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1089 - val_loss: 0.6051\n",
      "Epoch 130/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.1113 - val_loss: 0.6099\n",
      "Epoch 131/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.1135 - val_loss: 0.5981\n",
      "Epoch 132/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1092 - val_loss: 0.6158\n",
      "Epoch 133/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1057 - val_loss: 0.5783\n",
      "Epoch 134/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1039 - val_loss: 0.6116\n",
      "Epoch 135/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1068 - val_loss: 0.5975\n",
      "Epoch 136/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.1151 - val_loss: 0.5882\n",
      "Epoch 137/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1085 - val_loss: 0.6217\n",
      "Epoch 138/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0993 - val_loss: 0.6352\n",
      "Epoch 139/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1000 - val_loss: 0.5900\n",
      "Epoch 140/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1030 - val_loss: 0.5823\n",
      "Epoch 141/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0971 - val_loss: 0.5923\n",
      "Epoch 142/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0998 - val_loss: 0.6451\n",
      "Epoch 143/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0997 - val_loss: 0.5888\n",
      "Epoch 144/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0897 - val_loss: 0.6019\n",
      "Epoch 145/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1000 - val_loss: 0.6331\n",
      "Epoch 146/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1036 - val_loss: 0.6157\n",
      "Epoch 147/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0941 - val_loss: 0.6139\n",
      "Epoch 148/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0896 - val_loss: 0.5997\n",
      "Epoch 149/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0928 - val_loss: 0.5924\n",
      "Epoch 150/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0869 - val_loss: 0.6035\n",
      "Epoch 151/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0899 - val_loss: 0.6371\n",
      "Epoch 152/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0950 - val_loss: 0.6093\n",
      "Epoch 153/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0909 - val_loss: 0.5963\n",
      "Epoch 154/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.0808 - val_loss: 0.5950\n",
      "Epoch 155/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.0920 - val_loss: 0.5847\n",
      "Epoch 156/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0893 - val_loss: 0.6300\n",
      "Epoch 157/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0923 - val_loss: 0.5868\n",
      "Epoch 158/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0802 - val_loss: 0.6063\n",
      "Epoch 159/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0819 - val_loss: 0.6116\n",
      "Epoch 160/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0823 - val_loss: 0.5972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0833 - val_loss: 0.6101\n",
      "Epoch 162/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.0946 - val_loss: 0.6550\n",
      "Epoch 163/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0810 - val_loss: 0.6148\n",
      "Epoch 164/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0764 - val_loss: 0.6222\n",
      "Epoch 165/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0848 - val_loss: 0.5932\n",
      "Epoch 166/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0805 - val_loss: 0.6190\n",
      "Epoch 167/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0805 - val_loss: 0.6399\n",
      "Epoch 168/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0766 - val_loss: 0.6011\n",
      "Epoch 169/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.0787 - val_loss: 0.6337\n",
      "Epoch 170/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.0745 - val_loss: 0.6247\n",
      "Epoch 171/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.0804 - val_loss: 0.6430\n",
      "Epoch 172/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0789 - val_loss: 0.6344\n",
      "Epoch 173/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0793 - val_loss: 0.6274\n",
      "Epoch 174/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0790 - val_loss: 0.6340\n",
      "Epoch 175/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.0915 - val_loss: 0.6242\n",
      "Epoch 176/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0715 - val_loss: 0.6361\n",
      "Epoch 177/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0754 - val_loss: 0.6223\n",
      "Epoch 178/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0711 - val_loss: 0.6323\n",
      "Epoch 179/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0700 - val_loss: 0.6463\n",
      "Epoch 180/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.0739 - val_loss: 0.6308\n",
      "Epoch 181/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0712 - val_loss: 0.6492\n",
      "Epoch 182/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0863 - val_loss: 0.6504\n",
      "Epoch 183/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0785 - val_loss: 0.6372\n",
      "Epoch 184/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0743 - val_loss: 0.6260\n",
      "Epoch 185/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0714 - val_loss: 0.6262\n",
      "Epoch 186/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0643 - val_loss: 0.6264\n",
      "Epoch 187/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0707 - val_loss: 0.6187\n",
      "Epoch 188/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0789 - val_loss: 0.6464\n",
      "Epoch 189/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0739 - val_loss: 0.6506\n",
      "Epoch 190/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0634 - val_loss: 0.6272\n",
      "Epoch 191/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0640 - val_loss: 0.6435\n",
      "Epoch 192/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0654 - val_loss: 0.6609\n",
      "Epoch 193/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0666 - val_loss: 0.6441\n",
      "Epoch 194/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0670 - val_loss: 0.6695\n",
      "Epoch 195/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0662 - val_loss: 0.6555\n",
      "Epoch 196/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0714 - val_loss: 0.6427\n",
      "Epoch 197/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0664 - val_loss: 0.6444\n",
      "Epoch 198/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0576 - val_loss: 0.6750\n",
      "Epoch 199/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0644 - val_loss: 0.6478\n",
      "Epoch 200/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0607 - val_loss: 0.6397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d31941a8f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No modifique el código\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=200,\n",
    "          batch_size=32,\n",
    "          validation_split=0.2,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "descending-letters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 2ms/step - loss: 0.6637\n",
      "Test Loss: 0.663686215877533\n"
     ]
    }
   ],
   "source": [
    "# No modifique el código\n",
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-delivery",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "## Cuestión 2: Utilice el mismo modelo de la cuestión anterior pero añadiendo al menos dos técnicas distinas de regularización. No es necesario reducir el test loss.\n",
    "\n",
    "Ejemplos de regularización: [Prevent_Overfitting.ipynb](https://github.com/ezponda/intro_deep_learning/blob/main/class/Fundamentals/Prevent_Overfitting.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "hired-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "# Código aquí\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model = Sequential([\n",
    "    Dense(61, activation='relu', kernel_regularizer=l2(0.01), input_shape=(11,)),  # Añadir regularización L2\n",
    "    Dropout(0.5),  # Añadir Dropout\n",
    "    Dense(61, activation='relu', kernel_regularizer=l2(0.01)),  # Añadir regularización L2\n",
    "    Dropout(0.5),  # Añadir Dropout\n",
    "    Dense(61, activation='relu', kernel_regularizer=l2(0.01)),  # Añadir regularización L2\n",
    "    Dropout(0.5),  # Añadir Dropout\n",
    "    Dense(61, activation='relu', kernel_regularizer=l2(0.01)),  # Añadir regularización L2\n",
    "    Dropout(0.5),  # Añadir Dropout\n",
    "    Dense(1, activation='linear')  # Salida para regresión\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "focal-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del modelo\n",
    "# Código aquí\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "338f8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "prostate-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "122/122 [==============================] - 3s 7ms/step - loss: 15.3118 - val_loss: 8.3678\n",
      "Epoch 2/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 8.0376 - val_loss: 6.7140\n",
      "Epoch 3/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 6.1797 - val_loss: 6.0371\n",
      "Epoch 4/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 5.0691 - val_loss: 4.6884\n",
      "Epoch 5/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 4.4683 - val_loss: 3.4209\n",
      "Epoch 6/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 3.9066 - val_loss: 2.9987\n",
      "Epoch 7/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 3.5570 - val_loss: 2.4253\n",
      "Epoch 8/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 3.3754 - val_loss: 2.2746\n",
      "Epoch 9/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 3.1572 - val_loss: 1.9427\n",
      "Epoch 10/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 2.9070 - val_loss: 1.8357\n",
      "Epoch 11/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 2.7837 - val_loss: 1.7368\n",
      "Epoch 12/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 2.6618 - val_loss: 1.6983\n",
      "Epoch 13/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 2.5622 - val_loss: 1.5456\n",
      "Epoch 14/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 2.4662 - val_loss: 1.5202\n",
      "Epoch 15/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 2.2960 - val_loss: 1.3982\n",
      "Epoch 16/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 2.2265 - val_loss: 1.3220\n",
      "Epoch 17/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 2.1264 - val_loss: 1.3297\n",
      "Epoch 18/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 2.0675 - val_loss: 1.1905\n",
      "Epoch 19/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 2.0425 - val_loss: 1.1111\n",
      "Epoch 20/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 1.9210 - val_loss: 1.1278\n",
      "Epoch 21/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 1.8849 - val_loss: 1.0227\n",
      "Epoch 22/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 1.8815 - val_loss: 0.9971\n",
      "Epoch 23/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.8382 - val_loss: 0.9811\n",
      "Epoch 24/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.6785 - val_loss: 0.9668\n",
      "Epoch 25/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 1.6443 - val_loss: 0.9221\n",
      "Epoch 26/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.5464 - val_loss: 0.8422\n",
      "Epoch 27/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.5332 - val_loss: 0.8313\n",
      "Epoch 28/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.5249 - val_loss: 0.8303\n",
      "Epoch 29/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 1.4772 - val_loss: 0.7752\n",
      "Epoch 30/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 1.3973 - val_loss: 0.7961\n",
      "Epoch 31/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 1.4260 - val_loss: 0.7580\n",
      "Epoch 32/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.3490 - val_loss: 0.7185\n",
      "Epoch 33/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.3282 - val_loss: 0.7403\n",
      "Epoch 34/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.2851 - val_loss: 0.7176\n",
      "Epoch 35/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.2796 - val_loss: 0.7192\n",
      "Epoch 36/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 1.2525 - val_loss: 0.6657\n",
      "Epoch 37/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.2085 - val_loss: 0.6766\n",
      "Epoch 38/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.1807 - val_loss: 0.6458\n",
      "Epoch 39/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 1.1630 - val_loss: 0.6666\n",
      "Epoch 40/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 1.1614 - val_loss: 0.6668\n",
      "Epoch 41/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 1.1076 - val_loss: 0.6195\n",
      "Epoch 42/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 1.1103 - val_loss: 0.6178\n",
      "Epoch 43/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.1201 - val_loss: 0.6168\n",
      "Epoch 44/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 1.0817 - val_loss: 0.5924\n",
      "Epoch 45/200\n",
      "122/122 [==============================] - 1s 10ms/step - loss: 1.0719 - val_loss: 0.6115\n",
      "Epoch 46/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 1.0230 - val_loss: 0.6137\n",
      "Epoch 47/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.0382 - val_loss: 0.6054\n",
      "Epoch 48/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9820 - val_loss: 0.5792\n",
      "Epoch 49/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9883 - val_loss: 0.5984\n",
      "Epoch 50/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9846 - val_loss: 0.6173\n",
      "Epoch 51/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.9674 - val_loss: 0.5802\n",
      "Epoch 52/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9490 - val_loss: 0.5748\n",
      "Epoch 53/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9340 - val_loss: 0.6172\n",
      "Epoch 54/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9299 - val_loss: 0.6082\n",
      "Epoch 55/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9292 - val_loss: 0.5928\n",
      "Epoch 56/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9392 - val_loss: 0.5664\n",
      "Epoch 57/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9077 - val_loss: 0.5794\n",
      "Epoch 58/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9134 - val_loss: 0.5800\n",
      "Epoch 59/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8890 - val_loss: 0.5777\n",
      "Epoch 60/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.9004 - val_loss: 0.5629\n",
      "Epoch 61/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.8631 - val_loss: 0.5778\n",
      "Epoch 62/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8596 - val_loss: 0.5941\n",
      "Epoch 63/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8436 - val_loss: 0.5897\n",
      "Epoch 64/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.8273 - val_loss: 0.5705\n",
      "Epoch 65/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8374 - val_loss: 0.5951\n",
      "Epoch 66/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8144 - val_loss: 0.5963\n",
      "Epoch 67/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8441 - val_loss: 0.5796\n",
      "Epoch 68/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8244 - val_loss: 0.5930\n",
      "Epoch 69/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8026 - val_loss: 0.5997\n",
      "Epoch 70/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8021 - val_loss: 0.6038\n",
      "Epoch 71/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.7734 - val_loss: 0.5721\n",
      "Epoch 72/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7769 - val_loss: 0.5811\n",
      "Epoch 73/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.7992 - val_loss: 0.5961\n",
      "Epoch 74/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7608 - val_loss: 0.5901\n",
      "Epoch 75/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7762 - val_loss: 0.5856\n",
      "Epoch 76/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7708 - val_loss: 0.5992\n",
      "Epoch 77/200\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.7497 - val_loss: 0.5800\n",
      "Epoch 78/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.7426 - val_loss: 0.5776\n",
      "Epoch 79/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7608 - val_loss: 0.5781\n",
      "Epoch 80/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.7466 - val_loss: 0.5658\n",
      "Epoch 81/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.7481 - val_loss: 0.6210\n",
      "Epoch 82/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7494 - val_loss: 0.5931\n",
      "Epoch 83/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7309 - val_loss: 0.5934\n",
      "Epoch 84/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.7339 - val_loss: 0.5819\n",
      "Epoch 85/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7316 - val_loss: 0.5742\n",
      "Epoch 86/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7092 - val_loss: 0.5752\n",
      "Epoch 87/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7062 - val_loss: 0.6219\n",
      "Epoch 88/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6864 - val_loss: 0.5926\n",
      "Epoch 89/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7025 - val_loss: 0.6046\n",
      "Epoch 90/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.7110 - val_loss: 0.6097\n",
      "Epoch 91/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7036 - val_loss: 0.5651\n",
      "Epoch 92/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7059 - val_loss: 0.6352\n",
      "Epoch 93/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.7083 - val_loss: 0.5744\n",
      "Epoch 94/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6892 - val_loss: 0.5887\n",
      "Epoch 95/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6838 - val_loss: 0.5714\n",
      "Epoch 96/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6862 - val_loss: 0.5901\n",
      "Epoch 97/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6892 - val_loss: 0.5854\n",
      "Epoch 98/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6832 - val_loss: 0.5737\n",
      "Epoch 99/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6724 - val_loss: 0.5776\n",
      "Epoch 100/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6604 - val_loss: 0.5987\n",
      "Epoch 101/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6603 - val_loss: 0.5767\n",
      "Epoch 102/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6670 - val_loss: 0.5949\n",
      "Epoch 103/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6684 - val_loss: 0.5935\n",
      "Epoch 104/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6560 - val_loss: 0.5814\n",
      "Epoch 105/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6650 - val_loss: 0.5899\n",
      "Epoch 106/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6576 - val_loss: 0.5989\n",
      "Epoch 107/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6515 - val_loss: 0.5746\n",
      "Epoch 108/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6645 - val_loss: 0.5707\n",
      "Epoch 109/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6592 - val_loss: 0.5868\n",
      "Epoch 110/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6419 - val_loss: 0.5788\n",
      "Epoch 111/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6554 - val_loss: 0.5701\n",
      "Epoch 112/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6443 - val_loss: 0.5723\n",
      "Epoch 113/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6579 - val_loss: 0.5834\n",
      "Epoch 114/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6578 - val_loss: 0.5750\n",
      "Epoch 115/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6410 - val_loss: 0.5863\n",
      "Epoch 116/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6478 - val_loss: 0.5934\n",
      "Epoch 117/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6292 - val_loss: 0.5723\n",
      "Epoch 118/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6411 - val_loss: 0.5859\n",
      "Epoch 119/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6441 - val_loss: 0.5713\n",
      "Epoch 120/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6325 - val_loss: 0.5816\n",
      "Epoch 121/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6309 - val_loss: 0.5964\n",
      "Epoch 122/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6360 - val_loss: 0.5913\n",
      "Epoch 123/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6272 - val_loss: 0.5897\n",
      "Epoch 124/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6306 - val_loss: 0.5859\n",
      "Epoch 125/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6475 - val_loss: 0.6009\n",
      "Epoch 126/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6314 - val_loss: 0.5816\n",
      "Epoch 127/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6276 - val_loss: 0.5651\n",
      "Epoch 128/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6169 - val_loss: 0.5774\n",
      "Epoch 129/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6129 - val_loss: 0.5689\n",
      "Epoch 130/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6363 - val_loss: 0.5841\n",
      "Epoch 131/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6269 - val_loss: 0.5633\n",
      "Epoch 132/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6333 - val_loss: 0.5959\n",
      "Epoch 133/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6245 - val_loss: 0.5664\n",
      "Epoch 134/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6131 - val_loss: 0.5631\n",
      "Epoch 135/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6277 - val_loss: 0.6061\n",
      "Epoch 136/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6311 - val_loss: 0.5721\n",
      "Epoch 137/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.6254 - val_loss: 0.5885\n",
      "Epoch 138/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6233 - val_loss: 0.5840\n",
      "Epoch 139/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6168 - val_loss: 0.5657\n",
      "Epoch 140/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6286 - val_loss: 0.5857\n",
      "Epoch 141/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6138 - val_loss: 0.5845\n",
      "Epoch 142/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6183 - val_loss: 0.5714\n",
      "Epoch 143/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6106 - val_loss: 0.5703\n",
      "Epoch 144/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6038 - val_loss: 0.5802\n",
      "Epoch 145/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6044 - val_loss: 0.5626\n",
      "Epoch 146/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6171 - val_loss: 0.5681\n",
      "Epoch 147/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6060 - val_loss: 0.5767\n",
      "Epoch 148/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6205 - val_loss: 0.5664\n",
      "Epoch 149/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6104 - val_loss: 0.5606\n",
      "Epoch 150/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6081 - val_loss: 0.5645\n",
      "Epoch 151/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6240 - val_loss: 0.5659\n",
      "Epoch 152/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6129 - val_loss: 0.5620\n",
      "Epoch 153/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6088 - val_loss: 0.5714\n",
      "Epoch 154/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6159 - val_loss: 0.5778\n",
      "Epoch 155/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6120 - val_loss: 0.5718\n",
      "Epoch 156/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6094 - val_loss: 0.5675\n",
      "Epoch 157/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6053 - val_loss: 0.5790\n",
      "Epoch 158/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6165 - val_loss: 0.5702\n",
      "Epoch 159/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6104 - val_loss: 0.5766\n",
      "Epoch 160/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6073 - val_loss: 0.5678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6010 - val_loss: 0.5749\n",
      "Epoch 162/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6050 - val_loss: 0.5720\n",
      "Epoch 163/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6001 - val_loss: 0.5624\n",
      "Epoch 164/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6110 - val_loss: 0.5814\n",
      "Epoch 165/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6045 - val_loss: 0.5805\n",
      "Epoch 166/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.5982 - val_loss: 0.5661\n",
      "Epoch 167/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6108 - val_loss: 0.5621\n",
      "Epoch 168/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6043 - val_loss: 0.5612\n",
      "Epoch 169/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6063 - val_loss: 0.5654\n",
      "Epoch 170/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6090 - val_loss: 0.5605\n",
      "Epoch 171/200\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.6067 - val_loss: 0.5791\n",
      "Epoch 172/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6001 - val_loss: 0.5556\n",
      "Epoch 173/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6139 - val_loss: 0.5592\n",
      "Epoch 174/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6063 - val_loss: 0.5614\n",
      "Epoch 175/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6064 - val_loss: 0.5635\n",
      "Epoch 176/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5968 - val_loss: 0.5623\n",
      "Epoch 177/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5992 - val_loss: 0.5786\n",
      "Epoch 178/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6033 - val_loss: 0.5671\n",
      "Epoch 179/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5901 - val_loss: 0.5699\n",
      "Epoch 180/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6015 - val_loss: 0.5673\n",
      "Epoch 181/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6041 - val_loss: 0.5608\n",
      "Epoch 182/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6092 - val_loss: 0.5633\n",
      "Epoch 183/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6017 - val_loss: 0.5550\n",
      "Epoch 184/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6033 - val_loss: 0.5734\n",
      "Epoch 185/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6020 - val_loss: 0.5562\n",
      "Epoch 186/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.6045 - val_loss: 0.5509\n",
      "Epoch 187/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5933 - val_loss: 0.5716\n",
      "Epoch 188/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6094 - val_loss: 0.5573\n",
      "Epoch 189/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.5937 - val_loss: 0.5532\n",
      "Epoch 190/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.6068 - val_loss: 0.5613\n",
      "Epoch 191/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.5901 - val_loss: 0.5598\n",
      "Epoch 192/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6009 - val_loss: 0.5691\n",
      "Epoch 193/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.5949 - val_loss: 0.5639\n",
      "Epoch 194/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.5948 - val_loss: 0.5593\n",
      "Epoch 195/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.5943 - val_loss: 0.5644\n",
      "Epoch 196/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6019 - val_loss: 0.5608\n",
      "Epoch 197/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.5960 - val_loss: 0.5738\n",
      "Epoch 198/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.6088 - val_loss: 0.5735\n",
      "Epoch 199/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6045 - val_loss: 0.5781\n",
      "Epoch 200/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6099 - val_loss: 0.5485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d319431a20>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No modifique el código\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=200,\n",
    "          batch_size=batch_size,\n",
    "          validation_split=0.2,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "friendly-powell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 5ms/step - loss: 0.5791\n",
      "Test Loss: 0.5791063904762268\n"
     ]
    }
   ],
   "source": [
    "# No modifique el código\n",
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-vegetation",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "## Cuestión 3: Utilice el mismo modelo de la cuestión anterior pero añadiendo un callback de early stopping. No es necesario reducir el test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "precise-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "# Código aquí\n",
    "model = Sequential([\n",
    "    Dense(61, activation='relu', kernel_regularizer=l2(0.01), input_shape=(11,)),  # Añadir regularización L2\n",
    "    Dropout(0.5),  # Añadir Dropout\n",
    "    Dense(61, activation='relu', kernel_regularizer=l2(0.01)),  # Añadir regularización L2\n",
    "    Dropout(0.5),  # Añadir Dropout\n",
    "    Dense(61, activation='relu', kernel_regularizer=l2(0.01)),  # Añadir regularización L2\n",
    "    Dropout(0.5),  # Añadir Dropout\n",
    "    Dense(61, activation='relu', kernel_regularizer=l2(0.01)),  # Añadir regularización L2\n",
    "    Dropout(0.5),  # Añadir Dropout\n",
    "    Dense(1, activation='linear')  # Salida para regresión\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "blond-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilación del modelo\n",
    "# Código aquí\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "subsequent-roads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "122/122 [==============================] - 3s 8ms/step - loss: 12.7609 - val_loss: 8.6226\n",
      "Epoch 2/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 7.5262 - val_loss: 7.1211\n",
      "Epoch 3/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 5.8132 - val_loss: 4.9969\n",
      "Epoch 4/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 4.7951 - val_loss: 4.2543\n",
      "Epoch 5/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 4.1104 - val_loss: 3.2263\n",
      "Epoch 6/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 3.6144 - val_loss: 2.7683\n",
      "Epoch 7/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 3.3047 - val_loss: 2.4049\n",
      "Epoch 8/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 3.1305 - val_loss: 2.0253\n",
      "Epoch 9/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 2.9615 - val_loss: 1.9520\n",
      "Epoch 10/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 2.7437 - val_loss: 1.8050\n",
      "Epoch 11/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 2.6751 - val_loss: 1.8064\n",
      "Epoch 12/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 2.4318 - val_loss: 1.6490\n",
      "Epoch 13/200\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 2.4051 - val_loss: 1.6095\n",
      "Epoch 14/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 2.3033 - val_loss: 1.4783\n",
      "Epoch 15/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 2.1898 - val_loss: 1.3514\n",
      "Epoch 16/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 2.0916 - val_loss: 1.2730\n",
      "Epoch 17/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 2.0154 - val_loss: 1.2278\n",
      "Epoch 18/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.9401 - val_loss: 1.2094\n",
      "Epoch 19/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.8573 - val_loss: 1.1305\n",
      "Epoch 20/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.7746 - val_loss: 1.1232\n",
      "Epoch 21/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.7365 - val_loss: 1.0687\n",
      "Epoch 22/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.6507 - val_loss: 1.0402\n",
      "Epoch 23/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 1.6106 - val_loss: 0.9903\n",
      "Epoch 24/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.5686 - val_loss: 0.9000\n",
      "Epoch 25/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 1.5315 - val_loss: 0.8417\n",
      "Epoch 26/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.5296 - val_loss: 0.8374\n",
      "Epoch 27/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.4732 - val_loss: 0.8318\n",
      "Epoch 28/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.4619 - val_loss: 0.8142\n",
      "Epoch 29/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.3230 - val_loss: 0.7980\n",
      "Epoch 30/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.3305 - val_loss: 0.7583\n",
      "Epoch 31/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.2941 - val_loss: 0.7193\n",
      "Epoch 32/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 1.2912 - val_loss: 0.7013\n",
      "Epoch 33/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 1.2373 - val_loss: 0.7662\n",
      "Epoch 34/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 1.1994 - val_loss: 0.6592\n",
      "Epoch 35/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 1.1908 - val_loss: 0.6566\n",
      "Epoch 36/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 1.1832 - val_loss: 0.6589\n",
      "Epoch 37/200\n",
      "122/122 [==============================] - 1s 11ms/step - loss: 1.1548 - val_loss: 0.6583\n",
      "Epoch 38/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.0859 - val_loss: 0.6341\n",
      "Epoch 39/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.0415 - val_loss: 0.6184\n",
      "Epoch 40/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 1.1001 - val_loss: 0.6697\n",
      "Epoch 41/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.0384 - val_loss: 0.5918\n",
      "Epoch 42/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.0168 - val_loss: 0.6094\n",
      "Epoch 43/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 1.0575 - val_loss: 0.5819\n",
      "Epoch 44/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9753 - val_loss: 0.5958\n",
      "Epoch 45/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 1.0315 - val_loss: 0.5811\n",
      "Epoch 46/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.9588 - val_loss: 0.5828\n",
      "Epoch 47/200\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.9677 - val_loss: 0.6138\n",
      "Epoch 48/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9344 - val_loss: 0.6085\n",
      "Epoch 49/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.9376 - val_loss: 0.6230\n",
      "Epoch 50/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.9245 - val_loss: 0.5863\n",
      "Epoch 51/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9394 - val_loss: 0.5798\n",
      "Epoch 52/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.9244 - val_loss: 0.5882\n",
      "Epoch 53/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.9204 - val_loss: 0.5760\n",
      "Epoch 54/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.9047 - val_loss: 0.5800\n",
      "Epoch 55/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.8900 - val_loss: 0.5603\n",
      "Epoch 56/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8669 - val_loss: 0.5890\n",
      "Epoch 57/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.8651 - val_loss: 0.5720\n",
      "Epoch 58/200\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.8696 - val_loss: 0.5852\n",
      "Epoch 59/200\n",
      "122/122 [==============================] - 1s 9ms/step - loss: 0.8474 - val_loss: 0.6018\n",
      "Epoch 60/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8583 - val_loss: 0.5780\n",
      "Epoch 61/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8626 - val_loss: 0.5584\n",
      "Epoch 62/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.8246 - val_loss: 0.6102\n",
      "Epoch 63/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8304 - val_loss: 0.5659\n",
      "Epoch 64/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8303 - val_loss: 0.5768\n",
      "Epoch 65/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8168 - val_loss: 0.6072\n",
      "Epoch 66/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.8107 - val_loss: 0.5767\n",
      "Epoch 67/200\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.8060 - val_loss: 0.5914\n",
      "Epoch 68/200\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.7695 - val_loss: 0.5676\n",
      "Epoch 69/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7785 - val_loss: 0.5622\n",
      "Epoch 70/200\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.7724 - val_loss: 0.5854\n",
      "Epoch 71/200\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.7792 - val_loss: 0.5965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d31c11afe0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## definir el early stopping callback\n",
    "# Código aquí\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=200,\n",
    "          batch_size=32,\n",
    "          validation_split=0.2,\n",
    "          verbose=1,\n",
    "          callbacks=[early_stopping]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "pressing-object",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 3ms/step - loss: 0.6168\n",
      "Test Loss: 0.6167688965797424\n"
     ]
    }
   ],
   "source": [
    "# No modifique el código\n",
    "results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test Loss: {}'.format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-lesbian",
   "metadata": {},
   "source": [
    "<a name='1.4'></a>\n",
    "## Cuestión 4: ¿Podría haberse usado otra función de activación de la neurona de salida? En caso afirmativo especifíquela."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-silicon",
   "metadata": {},
   "source": [
    "Sí, se podría haber utilizado otra función de activación en la neurona de salida, dependiendo del problema que estemos tratando de resolver. En este caso, como estamos trabajando con un problema de regresión, la función de activación lineal (o ninguna, que es equivalente) es una elección común porque puede producir una gama de valores de salida real.\n",
    "\n",
    "Sin embargo, si tuviéramos un problema de clasificación binaria, podríamos usar la función de activación sigmoide, que devuelve un valor entre 0 y 1, lo que puede interpretarse como una probabilidad. Para problemas de clasificación multiclase, la función de activación softmax es una elección común para la capa de salida porque devuelve una distribución de probabilidad sobre las clases.\n",
    "\n",
    "Por lo tanto, aunque la función de activación lineal es una elección razonable para este problema, siempre es importante considerar el contexto del problema al elegir la función de activación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-christianity",
   "metadata": {},
   "source": [
    "<a name='1.5'></a>\n",
    "## Cuestión 5:  ¿Qué es lo que una neurona calcula?\n",
    "\n",
    "**a)** Una función de activación seguida de una suma ponderada  de las entradas.\n",
    "\n",
    "**b)** Una suma ponderada  de las entradas seguida de una función de activación.\n",
    "\n",
    "**c)** Una función de pérdida, definida sobre el target.\n",
    "\n",
    "**d)** Ninguna  de las anteriores es correcta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-burden",
   "metadata": {},
   "source": [
    "La respuesta es la **b) Una suma ponderada de las entradas seguida de una función de activación.**\n",
    "\n",
    "En una red neuronal, cada neurona recibe varias entradas y cada una de estas entradas se multiplica por un peso asociado. Estos productos se suman junto con un término de sesgo para producir la salida neta de la neurona. A esta salida neta se le aplica luego una función de activación para producir la salida final de la neurona. Por lo tanto, una neurona calcula una suma ponderada de sus entradas y luego aplica una función de activación a esta suma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-european",
   "metadata": {},
   "source": [
    "<a name='1.6'></a>\n",
    "## Cuestión 6:  ¿Cuál de estas funciones de activación no debería usarse en una capa oculta (hidden layer)?\n",
    "\n",
    "**a)** `sigmoid`\n",
    "\n",
    "**b)** `tanh`\n",
    "\n",
    "**c)** `relu`\n",
    "\n",
    "**d)** `linear`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-attack",
   "metadata": {},
   "source": [
    "La **d) linear**.\n",
    "\n",
    "Las funciones de activación lineales pueden ser útiles en la capa de salida en ciertos tipos de problemas de regresión, pero en las capas ocultas, no introducen la no linealidad necesaria para que la red aprenda de datos más complejos. En cambio, las funciones de activación como sigmoid, tanh y relu son comúnmente usadas en las capas ocultas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-utilization",
   "metadata": {},
   "source": [
    "<a name='1.7'></a>\n",
    "## Cuestión 7:  ¿Cuál de estas técnicas es efectiva para combatir el overfitting en una red con varias capas ocultas? Ponga todas las que lo sean.\n",
    "\n",
    "**a)** Dropout\n",
    "\n",
    "**b)** Regularización L2.\n",
    "\n",
    "**c)** Aumentar el tamaño del test set.\n",
    "\n",
    "**d)** Aumentar el tamaño del validation set.\n",
    "\n",
    "**e)** Reducir el número de capas de la red.\n",
    "\n",
    "**f)** Data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-trainer",
   "metadata": {},
   "source": [
    "Las técnicas que son efectivas para combatir el sobreajuste (overfitting) en una red con varias capas ocultas son:\n",
    "\n",
    "**a) Dropout:** Esta es una técnica de regularización en la que durante el entrenamiento, algunas neuronas de la red se “apagan” o se ignoran. Esto ayuda a prevenir el sobreajuste ya que obliga a la red a aprender características más robustas y generalizables.\n",
    "\n",
    "**b) Regularización L2:** Esta es otra técnica de regularización que añade un término de penalización a la función de pérdida basado en los pesos de las neuronas. Esto ayuda a prevenir el sobreajuste al desalentar a la red de aprender pesos demasiado grandes.\n",
    "\n",
    "**d) Aumentar el tamaño del validation set:** Un conjunto de validación más grande puede ayudar a detectar el sobreajuste más temprano durante el entrenamiento, ya que proporciona una mejor estimación del rendimiento del modelo en datos no vistos.\n",
    "\n",
    "**e) Reducir el número de capas de la red:** Una red con menos capas tiene menos parámetros y por lo tanto, es menos propensa al sobreajuste. Sin embargo, también puede ser menos capaz de aprender\n",
    "patrones complejos en los datos.\n",
    "\n",
    "**f) Data augmentation:** Esta técnica genera datos de entrenamiento adicionales al aplicar transformaciones aleatorias a las imágenes existentes, como rotaciones, desplazamientos, volteos, etc. Esto puede ayudar a prevenir el sobreajuste al proporcionar más variedad y cantidad de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-deposit",
   "metadata": {},
   "source": [
    "<a name='1.8'></a>\n",
    "## Cuestión 8:  Supongamos que queremos entrenar una red para un problema de clasificación de imágenes con las siguientes clases: {'perro','gato','persona'}. ¿Cuántas neuronas y que función de activación debería tener la capa de salida? ¿Qué función de pérdida (loss function) debería usarse?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-roulette",
   "metadata": {},
   "source": [
    "Para un problema de clasificación multiclase como en este caso, donde las clases son {‘perro’, ‘gato’, ‘persona’}, la capa de salida de la red neuronal debería tener 3 neuronas, una para cada clase.\n",
    "\n",
    "La función de activación más comúnmente utilizada en la capa de salida para problemas de clasificación multiclase es softmax. La función softmax convierte las salidas de las neuronas en probabilidades que suman 1, lo que permite interpretar las salidas como probabilidades para cada clase.\n",
    "\n",
    "En cuanto a la función de pérdida, la elección más común para problemas de clasificación multiclase es la entropía cruzada categórica (Categorical Cross-Entropy). Esta función de pérdida es adecuada porque mide la “distancia” entre la distribución de probabilidad predicha por el modelo y la distribución de probabilidad real (es decir, qué clase es la correcta). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
